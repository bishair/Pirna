{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ11OYoi8ivueBacjU9PAI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bishair/Pirna/blob/main/G10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKz4YKIm6D7F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from math import sqrt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file = files.upload()"
      ],
      "metadata": {
        "id": "A_behW_96QZz",
        "outputId": "0a5e6823-43f7-437d-81f7-7dd1375d29a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-881545ce-3abf-4d6f-92f8-b5530ba19a1e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-881545ce-3abf-4d6f-92f8-b5530ba19a1e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving groundwater.xlsx to groundwater.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the groundwater level data\n",
        "gw_data = pd.read_excel('groundwater.xlsx')\n",
        "gw_data.dropna(inplace=True)\n",
        "gw_data['Date'] = pd.to_datetime(gw_data['Date'], format='%d/%m/%Y %H:%M')\n",
        "print(gw_data)\n",
        "\n",
        "# Read the river water level data\n",
        "river_data = pd.read_excel('riverL.xlsx')\n",
        "river_data['Date'] = pd.to_datetime(river_data['Date'], format='%d/%m/%Y %H:%M')\n",
        "\n",
        "# Merge the two datasets on the 'Date' column\n",
        "merged_data = pd.merge_asof(gw_data.sort_values('Date'), river_data.sort_values('Date'), on='Date', direction='nearest')\n",
        "\n",
        "# Set 'Date' as the index\n",
        "merged_data.set_index('Date', inplace=True)\n",
        "\n",
        "resampled_data = merged_data.resample('H').ffill()\n",
        "resampled_data = resampled_data[1:]\n",
        "print(resampled_data)\n",
        "\n",
        "'''\n",
        "has_nan = resampled_data.isna().any().any()\n",
        "print(has_nan)\n",
        "total_nan_count = resampled_data.isna().sum().sum()\n",
        "print(total_nan_count)\n",
        "nan_per_column = resampled_data.isna().any()\n",
        "print(nan_per_column)\n",
        "nan_count_per_column = resampled_data.isna().sum()\n",
        "print(nan_count_per_column)'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "ipz0Z0UZ8FVn",
        "outputId": "7422f49f-b321-4a58-8421-8257646c4774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Date      G10\n",
            "0     2015-01-30 12:18:00  110.722\n",
            "1     2015-01-30 13:18:00  110.720\n",
            "2     2015-01-30 14:18:00  110.718\n",
            "3     2015-01-30 15:18:00  110.715\n",
            "4     2015-01-30 16:18:00  110.714\n",
            "...                   ...      ...\n",
            "17533 2017-01-30 10:19:00  109.456\n",
            "17534 2017-01-30 11:19:00  109.455\n",
            "17535 2017-01-30 12:19:00  109.456\n",
            "17536 2017-01-30 13:19:00  109.457\n",
            "17537 2017-01-30 14:19:00  109.459\n",
            "\n",
            "[17329 rows x 2 columns]\n",
            "                         G10  River\n",
            "Date                               \n",
            "2015-01-30 13:00:00  110.722  261.0\n",
            "2015-01-30 14:00:00  110.720  260.0\n",
            "2015-01-30 15:00:00  110.718  260.0\n",
            "2015-01-30 16:00:00  110.715  260.0\n",
            "2015-01-30 17:00:00  110.714  260.0\n",
            "...                      ...    ...\n",
            "2017-01-30 10:00:00  109.457  135.0\n",
            "2017-01-30 11:00:00  109.456  136.0\n",
            "2017-01-30 12:00:00  109.455  138.0\n",
            "2017-01-30 13:00:00  109.456  141.0\n",
            "2017-01-30 14:00:00  109.457  144.0\n",
            "\n",
            "[17546 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nhas_nan = resampled_data.isna().any().any()\\nprint(has_nan)\\ntotal_nan_count = resampled_data.isna().sum().sum()\\nprint(total_nan_count)\\nnan_per_column = resampled_data.isna().any()\\nprint(nan_per_column)\\nnan_count_per_column = resampled_data.isna().sum()\\nprint(nan_count_per_column)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 24\n",
        "\n",
        "# Split the original data first before scaling\n",
        "train_size = int(len(resampled_data) * 0.7)\n",
        "resampled_data_train = resampled_data[:train_size]\n",
        "resampled_data_test = resampled_data[train_size:]\n",
        "\n",
        "# Apply MinMaxScaler only to the training data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train_scaled = scaler.fit_transform(resampled_data_train)\n",
        "test_scaled = scaler.transform(resampled_data_test)\n",
        "\n",
        "joblib.dump(scaler, 'scaler.gz')\n",
        "\n",
        "\n",
        "# Function to create sequences\n",
        "def create_sequences(data, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(n_steps, len(data)):\n",
        "        X.append(data[i-n_steps:i, :])\n",
        "        y.append(data[i, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences for training and testing sets\n",
        "X_train, y_train = create_sequences(train_scaled, n_steps)\n",
        "X_test, y_test = create_sequences(test_scaled, n_steps)\n",
        "\n",
        "# Print shapes of X_train, y_train, X_test, y_test for verification\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n"
      ],
      "metadata": {
        "id": "Gohy8wy52sCU",
        "outputId": "e4cabb94-262e-4edd-a029-feff0e3af87c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (12258, 24, 2)\n",
            "y_train shape: (12258,)\n",
            "X_test shape: (5240, 24, 2)\n",
            "y_test shape: (5240,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train, X_train.shape)"
      ],
      "metadata": {
        "id": "80cC_yFQB9kt",
        "outputId": "7ef1bbc9-7ccd-492f-8063-7da9d597984d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.8510427  0.66666667]\n",
            "  [0.85004965 0.66287879]\n",
            "  [0.8490566  0.66287879]\n",
            "  ...\n",
            "  [0.83366435 0.65909091]\n",
            "  [0.8326713  0.65909091]\n",
            "  [0.83118173 0.66287879]]\n",
            "\n",
            " [[0.85004965 0.66287879]\n",
            "  [0.8490566  0.66287879]\n",
            "  [0.84756703 0.66287879]\n",
            "  ...\n",
            "  [0.8326713  0.65909091]\n",
            "  [0.83118173 0.66287879]\n",
            "  [0.83118173 0.66666667]]\n",
            "\n",
            " [[0.8490566  0.66287879]\n",
            "  [0.84756703 0.66287879]\n",
            "  [0.84707051 0.66287879]\n",
            "  ...\n",
            "  [0.83118173 0.66287879]\n",
            "  [0.83118173 0.66666667]\n",
            "  [0.82969215 0.67045455]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.34011917 0.25757576]\n",
            "  [0.33912612 0.25757576]\n",
            "  [0.33813307 0.25757576]\n",
            "  ...\n",
            "  [0.32025819 0.23863636]\n",
            "  [0.31926514 0.23863636]\n",
            "  [0.3182721  0.23863636]]\n",
            "\n",
            " [[0.33912612 0.25757576]\n",
            "  [0.33813307 0.25757576]\n",
            "  [0.33714002 0.25757576]\n",
            "  ...\n",
            "  [0.31926514 0.23863636]\n",
            "  [0.3182721  0.23863636]\n",
            "  [0.31777557 0.23863636]]\n",
            "\n",
            " [[0.33813307 0.25757576]\n",
            "  [0.33714002 0.25757576]\n",
            "  [0.33614697 0.25757576]\n",
            "  ...\n",
            "  [0.3182721  0.23863636]\n",
            "  [0.31777557 0.23863636]\n",
            "  [0.31727905 0.23863636]]] (12258, 24, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test, X_test.shape)"
      ],
      "metadata": {
        "id": "Cdes7NnzCBEy",
        "outputId": "251e0664-e495-44bb-ce74-8f5559d22725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.31578947 0.23484848]\n",
            "  [0.31479643 0.23484848]\n",
            "  [0.31479643 0.23484848]\n",
            "  ...\n",
            "  [0.2979146  0.23106061]\n",
            "  [0.29940417 0.23106061]\n",
            "  [0.30139027 0.23106061]]\n",
            "\n",
            " [[0.31479643 0.23484848]\n",
            "  [0.31479643 0.23484848]\n",
            "  [0.31380338 0.23863636]\n",
            "  ...\n",
            "  [0.29940417 0.23106061]\n",
            "  [0.30139027 0.23106061]\n",
            "  [0.30585899 0.23106061]]\n",
            "\n",
            " [[0.31479643 0.23484848]\n",
            "  [0.31380338 0.23863636]\n",
            "  [0.31330685 0.23106061]\n",
            "  ...\n",
            "  [0.30139027 0.23106061]\n",
            "  [0.30585899 0.23106061]\n",
            "  [0.30834161 0.23106061]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.24428997 0.21590909]\n",
            "  [0.2428004  0.21212121]\n",
            "  [0.2428004  0.20833333]\n",
            "  ...\n",
            "  [0.224429   0.18181818]\n",
            "  [0.22293942 0.18939394]\n",
            "  [0.2224429  0.19318182]]\n",
            "\n",
            " [[0.2428004  0.21212121]\n",
            "  [0.2428004  0.20833333]\n",
            "  [0.24230387 0.20454545]\n",
            "  ...\n",
            "  [0.22293942 0.18939394]\n",
            "  [0.2224429  0.19318182]\n",
            "  [0.22194638 0.20075758]]\n",
            "\n",
            " [[0.2428004  0.20833333]\n",
            "  [0.24230387 0.20454545]\n",
            "  [0.24180735 0.1969697 ]\n",
            "  ...\n",
            "  [0.2224429  0.19318182]\n",
            "  [0.22194638 0.20075758]\n",
            "  [0.2224429  0.21212121]]] (5240, 24, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train, y_train.shape)"
      ],
      "metadata": {
        "id": "U_p3UDnRDkO-",
        "outputId": "b1a7aef7-9aeb-487c-d6eb-7d692ec9e050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.83118173 0.82969215 0.82969215 ... 0.31777557 0.31727905 0.316286  ] (12258,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test, y_test.shape)"
      ],
      "metadata": {
        "id": "uCqVjDG-Dom2",
        "outputId": "6163a1a7-cc8a-45e4-fd2a-371efbf99811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.30585899 0.30834161 0.31330685 ... 0.22194638 0.2224429  0.22293942] (5240,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=(X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.regularizers import l1_l2\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(units=50, return_sequences=True, input_shape=input_shape,   #input shape => (timesteps, features).  X => (samples, timesteps, features)\n",
        "         dropout=0.2, recurrent_dropout=0.2,\n",
        "         kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(units=50, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64)\n"
      ],
      "metadata": {
        "id": "5ClHv6jDDro3",
        "outputId": "0e268942-b29e-46f5-d5c2-357a8fec4b81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "192/192 [==============================] - 22s 79ms/step - loss: 0.1454\n",
            "Epoch 2/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0084\n",
            "Epoch 3/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0060\n",
            "Epoch 4/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0053\n",
            "Epoch 5/50\n",
            "192/192 [==============================] - 18s 91ms/step - loss: 0.0051\n",
            "Epoch 6/50\n",
            "192/192 [==============================] - 16s 84ms/step - loss: 0.0048\n",
            "Epoch 7/50\n",
            "192/192 [==============================] - 16s 81ms/step - loss: 0.0044\n",
            "Epoch 8/50\n",
            "192/192 [==============================] - 15s 80ms/step - loss: 0.0043\n",
            "Epoch 9/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0040\n",
            "Epoch 10/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0037\n",
            "Epoch 11/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0039\n",
            "Epoch 12/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0037\n",
            "Epoch 13/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0036\n",
            "Epoch 14/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0034\n",
            "Epoch 15/50\n",
            "192/192 [==============================] - 15s 81ms/step - loss: 0.0035\n",
            "Epoch 16/50\n",
            "192/192 [==============================] - 16s 83ms/step - loss: 0.0033\n",
            "Epoch 17/50\n",
            "192/192 [==============================] - 16s 82ms/step - loss: 0.0034\n",
            "Epoch 18/50\n",
            "192/192 [==============================] - 15s 80ms/step - loss: 0.0034\n",
            "Epoch 19/50\n",
            "192/192 [==============================] - 15s 80ms/step - loss: 0.0031\n",
            "Epoch 20/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0034\n",
            "Epoch 21/50\n",
            "192/192 [==============================] - 15s 80ms/step - loss: 0.0032\n",
            "Epoch 22/50\n",
            "192/192 [==============================] - 15s 80ms/step - loss: 0.0032\n",
            "Epoch 23/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0030\n",
            "Epoch 24/50\n",
            "192/192 [==============================] - 15s 81ms/step - loss: 0.0032\n",
            "Epoch 25/50\n",
            "192/192 [==============================] - 16s 81ms/step - loss: 0.0029\n",
            "Epoch 26/50\n",
            "192/192 [==============================] - 16s 82ms/step - loss: 0.0030\n",
            "Epoch 27/50\n",
            "192/192 [==============================] - 16s 81ms/step - loss: 0.0029\n",
            "Epoch 28/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0030\n",
            "Epoch 29/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0029\n",
            "Epoch 30/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0028\n",
            "Epoch 31/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0028\n",
            "Epoch 32/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0028\n",
            "Epoch 33/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0028\n",
            "Epoch 34/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0028\n",
            "Epoch 35/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0030\n",
            "Epoch 36/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0028\n",
            "Epoch 37/50\n",
            "192/192 [==============================] - 15s 80ms/step - loss: 0.0027\n",
            "Epoch 38/50\n",
            "192/192 [==============================] - 16s 81ms/step - loss: 0.0028\n",
            "Epoch 39/50\n",
            "192/192 [==============================] - 16s 81ms/step - loss: 0.0028\n",
            "Epoch 40/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0028\n",
            "Epoch 41/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0028\n",
            "Epoch 42/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0027\n",
            "Epoch 43/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0027\n",
            "Epoch 44/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0026\n",
            "Epoch 45/50\n",
            "192/192 [==============================] - 15s 79ms/step - loss: 0.0027\n",
            "Epoch 46/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0027\n",
            "Epoch 47/50\n",
            "192/192 [==============================] - 15s 77ms/step - loss: 0.0027\n",
            "Epoch 48/50\n",
            "192/192 [==============================] - 15s 78ms/step - loss: 0.0027\n",
            "Epoch 49/50\n",
            "192/192 [==============================] - 16s 82ms/step - loss: 0.0027\n",
            "Epoch 50/50\n",
            "192/192 [==============================] - 15s 80ms/step - loss: 0.0026\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78550f151d20>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('lstmmodel.h5')\n",
        "model = load_model('lstmmodel.h5')\n",
        "scaler = joblib.load('scaler.gz')"
      ],
      "metadata": {
        "id": "NeRx0Clm6wkU",
        "outputId": "ffae3446-cb48-4f25-c249-fd691317c28e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrain the model with new data\n",
        "def retrain_model(model, X_new, y_new, epochs=5, batch_size=32):\n",
        "    # Partial retraining of the model with new data\n",
        "    model.fit(X_new, y_new, epochs=epochs, batch_size=batch_size, shuffle=True)\n",
        "    return model\n",
        "\n",
        "'''# Function to update y_train and recalculate statistics\n",
        "def update_training_data(new_data, y_train, max_length=10000):\n",
        "    updated_y_train = np.append(y_train, new_data)\n",
        "    if len(updated_y_train) > max_length:\n",
        "        updated_y_train = updated_y_train[-max_length:]\n",
        "    mean_y_train = np.mean(updated_y_train)\n",
        "    std_y_train = np.std(updated_y_train)\n",
        "    return updated_y_train, mean_y_train, std_y_train '''"
      ],
      "metadata": {
        "id": "SvQCwgRqrEYq",
        "outputId": "85a72114-b333-4e4b-d892-9c4a974b5042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Function to update y_train and recalculate statistics\\ndef update_training_data(new_data, y_train, max_length=10000):\\n    updated_y_train = np.append(y_train, new_data)\\n    if len(updated_y_train) > max_length:\\n        updated_y_train = updated_y_train[-max_length:]\\n    mean_y_train = np.mean(updated_y_train)\\n    std_y_train = np.std(updated_y_train)\\n    return updated_y_train, mean_y_train, std_y_train '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "window_size = 24\n",
        "\n",
        "actual_values = []\n",
        "predicted_values = []\n",
        "errors = []\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    X_input = X_test[i, :, :]            #selecting a single sequence from X_test, X_input  have the shape [timesteps, features] => (24, 2)\n",
        "    X_input = np.reshape(X_input, (1, X_input.shape[0], X_input.shape[1]))  #LSTM models  expect input in the form of 3D array [batch_size, timesteps, features] => (1, 24, 2)\n",
        "\n",
        "    # Make a prediction\n",
        "    forecast = model.predict(X_input)    #shapeof forecast => (1 timesequence of 24 steps,1 feature) => e.g [[0.28847086]]\n",
        "    dummy = np.zeros((1, 2))\n",
        "    dummy[:, 0] = forecast[:, 0]\n",
        "    forecasted_value = scaler.inverse_transform(dummy)[0, 0]\n",
        "    predicted_values.append(forecasted_value)    #Forecasted value inverse transformed => e.g 109.58\n",
        "\n",
        "    actual = y_test[i] if i < len(y_test) else None   #fetching the observed value corresponding to the i-th sequence in X_test\n",
        "    if actual is not None:\n",
        "        actual_transformed = scaler.inverse_transform([[actual, 0]])[0, 0]\n",
        "        actual_values.append(actual_transformed)     #Observed value => e.g 109.639\n",
        "\n",
        "    error = abs(forecasted_value - actual_transformed)\n",
        "    errors.append(error)\n",
        "\n",
        "\n",
        "    ax.clear()\n",
        "    ax.plot(actual_values, label='Actual', color='blue')\n",
        "    ax.plot(predicted_values, label='Predicted', color='red')\n",
        "    ax.set_title('Groundwater Level Prediction')\n",
        "    ax.set_xlabel('Hours')\n",
        "    ax.set_ylabel('Water Level')\n",
        "    ax.legend()\n",
        "\n",
        "    if len(errors) >= window_size:\n",
        "        windowed_errors = errors[-window_size:]\n",
        "        mean_error = np.mean(windowed_errors)\n",
        "        std_error = np.std(windowed_errors)\n",
        "        threshold = mean_error + 3 * std_error\n",
        "\n",
        "        windowed_anomalies = [j for j, e in enumerate(windowed_errors) if e > threshold]\n",
        "        if windowed_anomalies:\n",
        "            print(f\"Anomalies detected at indices: {[j + i - window_size + 1 for j in windowed_anomalies]}\")\n",
        "            # Retrain the model with the most recent window of data\n",
        "            #model = retrain_model(model, X_test[i-window_size+1:i+1], y_test[i-window_size+1:i+1])\n",
        "\n",
        "\n",
        "    # Display the plot\n",
        "    display(fig)\n",
        "    plt.pause(0.5)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "# Hide the plot for the next cell\n",
        "plt.close(fig)\n"
      ],
      "metadata": {
        "id": "mxNHComhzxXL",
        "outputId": "15d9a068-3bdb-41fb-dbdf-5868ec4c049d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f2e2f55597dc>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#shapeof forecast => (1 timesequence of 24 steps,1 feature) => e.g [[0.28847086]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdummy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2618\u001b[0m                     )\n\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2620\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   2621\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorExactEvalDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1293\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2323\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2326\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_FlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m     34\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    301\u001b[0m   )\n\u001b[1;32m    302\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mplaceholder_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mplaceholder_arguments\u001b[0;34m(self, placeholder_context)\u001b[0m\n\u001b[1;32m    354\u001b[0m                          \"partially defined function type.\")\n\u001b[1;32m    355\u001b[0m       \u001b[0mplaceholder_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_naming_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       arguments[parameter.name] = parameter.type_constraint.placeholder_value(\n\u001b[0m\u001b[1;32m    357\u001b[0m           placeholder_context)\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36mplaceholder_value\u001b[0;34m(self, placeholder_context)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mplaceholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0mplaceholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m_graph_placeholder\u001b[0;34m(self, graph, name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       op = graph._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   1061\u001b[0m           \u001b[0;34m\"Placeholder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           attrs=attrs, name=name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m     return super()._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         compute_device)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   2650\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2652\u001b[0;31m       ret = Operation.from_node_def(\n\u001b[0m\u001b[1;32m   2653\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mfrom_node_def\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# Initialize c_op from node_def and other inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_c_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_input_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymbolicTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1024\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mextract_traceback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m     pywrap_tf_session.TF_SetOpStackTrace(\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_stack.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(stacklevel)\u001b[0m\n\u001b[1;32m    160\u001b[0m   \"\"\"\n\u001b[1;32m    161\u001b[0m   \u001b[0mthread_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_thread_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m   return _tf_stack.extract_stack(\n\u001b[0m\u001b[1;32m    163\u001b[0m       \u001b[0m_source_mapper_stacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthread_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0m_source_filter_stacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthread_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from scipy.stats import percentileofscore\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "actual_values = []\n",
        "predicted_values = []\n",
        "errors = []\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    X_input = X_test[i, :, :]  #selecting a single sequence from X_test, X_input  have the shape [timesteps, features] => (24, 2)\n",
        "    X_input = np.reshape(X_input, (1, X_input.shape[0], X_input.shape[1]))\n",
        "\n",
        "    # Predicted values\n",
        "    forecast = model.predict(X_input)\n",
        "    dummy_forecast = np.zeros((1, 2))\n",
        "    dummy_forecast[:, 0] = forecast[:, 0]\n",
        "    forecasted_value = scaler.inverse_transform(dummy_forecast)[0, 0]\n",
        "    predicted_values.append(forecasted_value)\n",
        "\n",
        "    # Observed values and Error calculation\n",
        "    if i < len(y_test):\n",
        "        actual = y_test[i]\n",
        "        dummy_actual = np.zeros((1, 2))\n",
        "        dummy_actual[:, 0] = actual\n",
        "        actual_transformed = scaler.inverse_transform(dummy_actual)[0, 0]\n",
        "        actual_values.append(actual_transformed)\n",
        "\n",
        "        # Calculate absolute error\n",
        "        error = abs(forecasted_value - actual_transformed)\n",
        "        errors.append(error)\n",
        "\n",
        "        # Define a threshold for anomaly detection\n",
        "        if len(errors) > 0:\n",
        "          mean_error = np.mean(errors)\n",
        "          std_error = np.std(errors)\n",
        "          threshold = mean_error + 2 * std_error\n",
        "\n",
        "        # Identify anomalies\n",
        "        anomalies = [i for i, e in enumerate(errors) if e > threshold]\n",
        "        #if anomalies:\n",
        "          #print(f\"Anomalies detected at indices: {anomalies}\")\n",
        "          #retrain_model(model, X_train, y_train)  # Call retrain model function if anomalies are detected\n",
        "\n",
        "    # Update the plot\n",
        "    ax.clear()\n",
        "    ax.scatter(range(len(actual_values)), actual_values, label='Observed', color='blue')\n",
        "    ax.scatter(range(len(predicted_values)), predicted_values, label='Predicted', color='red')\n",
        "    ax.set_title('Groundwater Level Prediction')\n",
        "    ax.set_xlabel('Hours')\n",
        "    ax.set_ylabel('Water Level')\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "    # Calculate overall MSE and display it\n",
        "    if len(actual_values) > 0 and len(predicted_values) > 0:\n",
        "        overall_mse = mean_squared_error(actual_values, predicted_values)\n",
        "        # Display MSE on the plot\n",
        "        ax.text(0.01, 0.95, f'MSE: {overall_mse:.4f}', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
        "    display(fig)\n",
        "    plt.pause(0.5)\n",
        "    clear_output(wait=True)\n",
        "\n",
        "# Close the plot\n",
        "plt.close(fig)'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ueqbkQXn4CR",
        "outputId": "6a705542-7a7f-45fd-ee53-3adffc7e108e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from scipy.stats import percentileofscore\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import mean_squared_error\\nfrom IPython.display import display, clear_output\\n\\n\\nactual_values = []\\npredicted_values = []\\nerrors = []\\n\\n\\nfig, ax = plt.subplots(figsize=(10, 6))\\n\\nfor i in range(X_test.shape[0]):\\n    X_input = X_test[i, :, :]  #selecting a single sequence from X_test, X_input  have the shape [timesteps, features] => (24, 2)\\n    X_input = np.reshape(X_input, (1, X_input.shape[0], X_input.shape[1]))  #LSTM models  expect input in the form of 3D array [batch_size, timesteps, features] => (1, 24, 2)\\n\\n    # Predicted values\\n    forecast = model.predict(X_input)  #shapeof forecast => (1 timesequence of 24 steps,1 feature) => e.g [[0.28847086]]\\n    dummy_forecast = np.zeros((1, 2))\\n    dummy_forecast[:, 0] = forecast[:, 0]\\n    forecasted_value = scaler.inverse_transform(dummy_forecast)[0, 0]\\n    predicted_values.append(forecasted_value)   #Forecasted value inverse transformed => e.g 109.58\\n\\n    # Observed values and Error calculation\\n    if i < len(y_test):\\n        actual = y_test[i]     #fetching the observed value corresponding to the i-th sequence in X_test\\n        dummy_actual = np.zeros((1, 2))\\n        dummy_actual[:, 0] = actual\\n        actual_transformed = scaler.inverse_transform(dummy_actual)[0, 0]\\n        actual_values.append(actual_transformed)   #Observed value => e.g 109.639\\n\\n        # Calculate absolute error\\n        error = abs(forecasted_value - actual_transformed)\\n        errors.append(error)\\n\\n        # Define a threshold for anomaly detection\\n        if len(errors) > 0:\\n          mean_error = np.mean(errors)\\n          std_error = np.std(errors)\\n          threshold = mean_error + 2 * std_error\\n\\n        # Identify anomalies\\n        anomalies = [i for i, e in enumerate(errors) if e > threshold]\\n        #if anomalies:\\n          #print(f\"Anomalies detected at indices: {anomalies}\")\\n          #retrain_model(model, X_train, y_train)  # Call retrain model function if anomalies are detected\\n\\n    # Update the plot\\n    ax.clear()\\n    ax.scatter(range(len(actual_values)), actual_values, label=\\'Observed\\', color=\\'blue\\')\\n    ax.scatter(range(len(predicted_values)), predicted_values, label=\\'Predicted\\', color=\\'red\\')\\n    ax.set_title(\\'Groundwater Level Prediction\\')\\n    ax.set_xlabel(\\'Hours\\')\\n    ax.set_ylabel(\\'Water Level\\')\\n    ax.legend()\\n\\n\\n    # Calculate overall MSE and display it\\n    if len(actual_values) > 0 and len(predicted_values) > 0:\\n        overall_mse = mean_squared_error(actual_values, predicted_values)\\n        # Display MSE on the plot\\n        ax.text(0.01, 0.95, f\\'MSE: {overall_mse:.4f}\\', transform=ax.transAxes, fontsize=12, verticalalignment=\\'top\\')\\n    display(fig)\\n    plt.pause(0.5)\\n    clear_output(wait=True)\\n\\n# Close the plot\\nplt.close(fig)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from scipy.stats import percentileofscore\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "actual_values = []\n",
        "predicted_values = []\n",
        "anomalies = []\n",
        "\n",
        "# Inverse transform y_train to original scale\n",
        "y_train_inversed = scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Recalculate mean and standard deviation on the inversed data\n",
        "mean_y_train = np.mean(y_train_inversed)\n",
        "std_y_train = np.std(y_train_inversed)\n",
        "threshold = 3  # Anomaly detection threshold\n",
        "\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    X_input = X_test[i, :, :]       #selecting a single sequence from X_test, X_input  have the shape [timesteps, features] => (24, 2)\n",
        "    X_input = np.reshape(X_input, (1, X_input.shape[0], X_input.shape[1]))    #LSTM models  expect input in the form of 3D array [batch_size, timesteps, features] => (1, 24, 2)\n",
        "\n",
        "    # Make a prediction and store prediction in predicted_values array\n",
        "    forecast = model.predict(X_input)     #shapeof forecast => (1 timesequence of 24 steps,1 feature) => e.g [[0.28847086]]\n",
        "    dummy_forecast = np.zeros((1, 2))\n",
        "    dummy_forecast[:, 0] = forecast[:, 0]\n",
        "    forecasted_value = scaler.inverse_transform(dummy_forecast)[0, 0]\n",
        "    predicted_values.append(forecasted_value)    #Forecasted value => e.g 109.58\n",
        "\n",
        "\n",
        "    # Process observed values for comparison and store in actual_values array\n",
        "    if i < len(y_test):\n",
        "        actual = y_test[i]    #fetching the observed value corresponding to the i-th sequence in X_test\n",
        "        dummy_actual = np.zeros((1, 2))\n",
        "        dummy_actual[:, 0] = actual\n",
        "        actual_transformed = scaler.inverse_transform(dummy_actual)[0, 0]\n",
        "        actual_values.append(actual_transformed)        #Observed value => e.g 109.639\n",
        "\n",
        "        if np.abs(actual_transformed - mean_y_train) > threshold * std_y_train:\n",
        "            print(f\"Significant change detected at index {i}, triggering retraining.\")\n",
        "            model = retrain_model(model, X_train, y_train)  # Retrain the model\n",
        "            y_train, mean_y_train, std_y_train = update_training_data(y_test[i], y_train)  # Update y_train\n",
        "\n",
        "        if anomaly_detected:\n",
        "          # Preprocess new data for model retraining\n",
        "          #X_new, y_new = preprocess_new_data(new_data)\n",
        "          # Retrain the model with new data\n",
        "          X_new, y_new = X_test[:i+1, :, :], y_test[:i+1]  # Use data up to the current point\n",
        "          model = retrain_model(model, X_new, y_new, epochs=additional_epochs, batch_size=batch_size)\n",
        "          # Update y_train with the new data and recalculate statistics\n",
        "          y_train, mean_y_train, std_y_train = update_training_data(y_test[i], y_train)\n",
        "\n",
        "        # Update the plot\n",
        "        ax.clear()\n",
        "        ax.plot(actual_values, label='Observed', color='blue')\n",
        "        ax.plot(predicted_values, label='Predicted', color='red')\n",
        "        ax.set_title('Groundwater Level Prediction')\n",
        "        ax.set_xlabel('Hours')\n",
        "        ax.set_ylabel('Water Level')\n",
        "        ax.legend()\n",
        "        display(fig)\n",
        "        plt.pause(0.5)\n",
        "        clear_output(wait=True)\n",
        "\n",
        "\n",
        "plt.close(fig)'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yHbCK5kpKAIU",
        "outputId": "b2a5a226-ae8a-4df8-8737-4972966a98ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from scipy.stats import percentileofscore\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom IPython.display import display, clear_output\\n\\n\\nactual_values = []\\npredicted_values = []\\nanomalies = []\\n\\n# Inverse transform y_train to original scale\\ny_train_inversed = scaler.inverse_transform(y_train.reshape(-1, 1)).flatten()\\n\\n# Recalculate mean and standard deviation on the inversed data\\nmean_y_train = np.mean(y_train_inversed)\\nstd_y_train = np.std(y_train_inversed)\\nthreshold = 3  # Anomaly detection threshold\\n\\n\\nfor i in range(X_test.shape[0]):\\n    X_input = X_test[i, :, :]       #selecting a single sequence from X_test, X_input  have the shape [timesteps, features] => (24, 2)\\n    X_input = np.reshape(X_input, (1, X_input.shape[0], X_input.shape[1]))    #LSTM models  expect input in the form of 3D array [batch_size, timesteps, features] => (1, 24, 2)\\n\\n    # Make a prediction and store prediction in predicted_values array\\n    forecast = model.predict(X_input)     #shapeof forecast => (1 timesequence of 24 steps,1 feature) => e.g [[0.28847086]]\\n    dummy_forecast = np.zeros((1, 2))\\n    dummy_forecast[:, 0] = forecast[:, 0]\\n    forecasted_value = scaler.inverse_transform(dummy_forecast)[0, 0]\\n    predicted_values.append(forecasted_value)    #Forecasted value => e.g 109.58\\n\\n\\n    # Process observed values for comparison and store in actual_values array\\n    if i < len(y_test):\\n        actual = y_test[i]    #fetching the observed value corresponding to the i-th sequence in X_test\\n        dummy_actual = np.zeros((1, 2))\\n        dummy_actual[:, 0] = actual\\n        actual_transformed = scaler.inverse_transform(dummy_actual)[0, 0]\\n        actual_values.append(actual_transformed)        #Observed value => e.g 109.639\\n\\n        if np.abs(actual_transformed - mean_y_train) > threshold * std_y_train:\\n            print(f\"Significant change detected at index {i}, triggering retraining.\")\\n            model = retrain_model(model, X_train, y_train)  # Retrain the model\\n            y_train, mean_y_train, std_y_train = update_training_data(y_test[i], y_train)  # Update y_train\\n\\n        if anomaly_detected:\\n          # Preprocess new data for model retraining\\n          #X_new, y_new = preprocess_new_data(new_data)  # You should define this function based on your data preprocessing steps\\n          # Retrain the model with new data\\n          X_new, y_new = X_test[:i+1, :, :], y_test[:i+1]  # Use data up to the current point\\n          model = retrain_model(model, X_new, y_new, epochs=additional_epochs, batch_size=batch_size)\\n          # Update y_train with the new data and recalculate statistics\\n          y_train, mean_y_train, std_y_train = update_training_data(y_test[i], y_train)\\n\\n        # Update the plot\\n        ax.clear()\\n        ax.plot(actual_values, label=\\'Observed\\', color=\\'blue\\')\\n        ax.plot(predicted_values, label=\\'Predicted\\', color=\\'red\\')\\n        ax.set_title(\\'Groundwater Level Prediction\\')\\n        ax.set_xlabel(\\'Hours\\')\\n        ax.set_ylabel(\\'Water Level\\')\\n        ax.legend()\\n        display(fig)\\n        plt.pause(0.5)\\n        clear_output(wait=True)\\n\\n\\nplt.close(fig)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from scipy.stats import percentileofscore\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "print(\"y_train range: min =\", np.min(y_train), \", max =\", np.max(y_train))\n",
        "\n",
        "\n",
        "#fdc_percentiles = np.percentile(y_train, np.arange(0, 101, 1))\n",
        "\n",
        "\n",
        "actual_values = []\n",
        "predicted_values = []\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    X_input = X_test[i, :, :]\n",
        "    X_input = np.reshape(X_input, (1, X_input.shape[0], X_input.shape[1]))\n",
        "\n",
        "    # Predicted values\n",
        "    forecast = model.predict(X_input)\n",
        "    dummy_forecast = np.zeros((1, 2))\n",
        "    dummy_forecast[:, 0] = forecast[:, 0]\n",
        "    forecasted_value = scaler.inverse_transform(dummy_forecast)[0, 0]\n",
        "    predicted_values.append(forecasted_value)\n",
        "\n",
        "    # Observed values\n",
        "    if i < len(y_test):\n",
        "        actual = y_test[i]\n",
        "        dummy_actual = np.zeros((1, 2))\n",
        "        dummy_actual[:, 0] = actual\n",
        "        actual_transformed = scaler.inverse_transform(dummy_actual)[0, 0]\n",
        "        actual_values.append(actual_transformed)\n",
        "\n",
        "        # Determine the percentile of the prediction in the FDC\n",
        "        #prediction_percentile = percentileofscore(fdc_percentiles, forecasted_value)\n",
        "        #if prediction_percentile < 5 or prediction_percentile > 95:\n",
        "         #   anomalies.append(i)\n",
        "\n",
        "    # Update the plot\n",
        "    ax.clear()\n",
        "    ax.plot(actual_values, label='Observed', color='blue')\n",
        "    ax.plot(predicted_values, label='Predicted', color='red')\n",
        "    #if anomalies:\n",
        "    #    ax.scatter(anomalies, [actual_values[j] for j in anomalies], color='orange', label='Anomalies')\n",
        "    ax.set_title('Groundwater Level Prediction')\n",
        "    ax.set_xlabel('Hours')\n",
        "    ax.set_ylabel('Water Level')\n",
        "    ax.legend()\n",
        "    display(fig)\n",
        "    plt.pause(0.5)\n",
        "    clear_output(wait=True)\n",
        "\n",
        "plt.close(fig)\n",
        "\n",
        "forecasted_values = np.array(predicted_values)\n",
        "print(\"Forecasted range: min =\", np.min(forecasted_values), \", max =\", np.max(forecasted_values))'''\n",
        "\n"
      ],
      "metadata": {
        "id": "MYdwLfllMlaL",
        "outputId": "40f40f3d-0cb5-4e8d-ca56-0b4af02ef4a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from scipy.stats import percentileofscore\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom IPython.display import display, clear_output\\n\\n\\nprint(\"y_train range: min =\", np.min(y_train), \", max =\", np.max(y_train))\\n\\n\\n#fdc_percentiles = np.percentile(y_train, np.arange(0, 101, 1))\\n\\n\\nactual_values = []\\npredicted_values = []\\n\\n\\nfig, ax = plt.subplots(figsize=(10, 6))\\n\\nfor i in range(X_test.shape[0]):\\n    X_input = X_test[i, :, :]\\n    X_input = np.reshape(X_input, (1, X_input.shape[0], X_input.shape[1]))\\n\\n    # Predicted values\\n    forecast = model.predict(X_input)\\n    dummy_forecast = np.zeros((1, 2))\\n    dummy_forecast[:, 0] = forecast[:, 0]\\n    forecasted_value = scaler.inverse_transform(dummy_forecast)[0, 0]\\n    predicted_values.append(forecasted_value)\\n\\n    # Observed values\\n    if i < len(y_test):\\n        actual = y_test[i]\\n        dummy_actual = np.zeros((1, 2))\\n        dummy_actual[:, 0] = actual\\n        actual_transformed = scaler.inverse_transform(dummy_actual)[0, 0]\\n        actual_values.append(actual_transformed)\\n\\n        # Determine the percentile of the prediction in the FDC\\n        #prediction_percentile = percentileofscore(fdc_percentiles, forecasted_value)\\n        #if prediction_percentile < 5 or prediction_percentile > 95:\\n         #   anomalies.append(i)\\n\\n    # Update the plot\\n    ax.clear()\\n    ax.plot(actual_values, label=\\'Observed\\', color=\\'blue\\')\\n    ax.plot(predicted_values, label=\\'Predicted\\', color=\\'red\\')\\n    #if anomalies:\\n    #    ax.scatter(anomalies, [actual_values[j] for j in anomalies], color=\\'orange\\', label=\\'Anomalies\\')\\n    ax.set_title(\\'Groundwater Level Prediction\\')\\n    ax.set_xlabel(\\'Hours\\')\\n    ax.set_ylabel(\\'Water Level\\')\\n    ax.legend()\\n    display(fig)\\n    plt.pause(0.5)\\n    clear_output(wait=True)\\n\\nplt.close(fig)\\n\\nforecasted_values = np.array(predicted_values)\\nprint(\"Forecasted range: min =\", np.min(forecasted_values), \", max =\", np.max(forecasted_values))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4vkKqJTPOZCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}